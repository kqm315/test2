# -*- coding: utf-8 -*-
#import requests
import pandas as pd
import time
import re

from selenium import webdriver
from bs4 import BeautifulSoup
#from selenium.webdriver.common.keys import Keys

#https://epubs.siam.org/action/doSearch?AllField=machine+learning&publication=40000050&target=default&startPage=0&sortBy=Ppub
#start_url="http://ieeexplore.ieee.org/search/searchresult.jsp?"
#query=['machine learning']
#sortType='desc_p_Publication_Year'
#url=start_url+'queryText='+query[0]+'&sortType='+sortType

def get_html(url):
    driver=webdriver.Firefox()
    driver.get(url)
    
    time.sleep(5)
#鍔犺浇婊氬姩鍐呭锛屾瘡椤?5鏉￠渶瑕佹粴鍔ㄤ袱娆?
    js="var q=document.documentElement.scrollTop=10000"  
    driver.execute_script(js) 
    time.sleep(6)
  #  driver.execute_script(js) 
 #   time.sleep(5)
    driver.execute_script(js) 
#    time.sleep(5)
    html=driver.page_source
    driver.close()
    return html

def parse(html):
    soup=BeautifulSoup(html)
    
    title_list=[]
    author_list=[]
    articleUrl_list=[]
    publish_list=[]
    
    articles=soup.find_all('div',attrs={'class':'notSelectedRow'})
  #  print(str(articles[0]))
  #  print(str(articles[1]))
#    print(article)
    for article in articles:
        try:
            title=article.find('span',attrs={'class':'hlFld-Title'}).text#瀵绘壘姣忎釜鏂囩珷鏍囩鍐呯殑鎸囧畾鍐呭
            author=article.find('a',attrs={'class':'entryAuthor'}).text
            temp=article.find('a',attrs={'class':'ref nowrap'})
            articleUrl='https://epubs.siam.org'+temp['href']
            publish=article.find('a',attrs={'class':'searchResultJournal'}).text
            title_list.append(title)
            author_list.append(author)
            articleUrl_list.append(articleUrl)
            publish_list.append(publish)
            print(articleUrl)
        except:
            print('杩欎釜缃戝潃鍐呭鏈夐棶棰橈細'+articleUrl)
    return title_list,author_list,publish_list,articleUrl_list   
    '''
    soup=BeautifulSoup(html)
    
    name_list=[]
    author_list=[]
    articleUrl_list=[]
    publish_list=[]
    
    articles=soup.find('div',attrs={'class':'article-list List-results ng-isolate-scope'})
    for article in articles.find_all('div',attrs={'class':'List-results-items ng-scope'}):
        temp=article.find('a',attrs={'class':'ng-binding ng-scope'})
        name=temp.getText()
        articleUrl=temp['href']        
        author=article.find('p',attrs={'class':'author ng-scope'}).getText()
        publish=article.find('div',attrs={'class':'description u-mb-1'}).getText()
        
        name_list.append(name)
        author_list.append(author)
        articleUrl_list.append('http://ieeexplore.ieee.org'+articleUrl)
        publish_list.append(publish)
        #print(name)
    
    return name_list,author_list,publish_list,articleUrl_list
'''
def get_abstract(name,url):
    length=len(url)
    string=''
    
    for i in range(length):
        try:
            driver=webdriver.Firefox()
            driver.get(url[i])
            a_html=driver.page_source
            driver.close()
            soup=BeautifulSoup(a_html)
            text=soup.find('div',attrs={'class':'abstractSection'}).getText()
            print('鍐呭_________'+str(text))
            name[i]=re.sub(r':',' ',name[i])
            print('鏍囬_________'+name[i])
            document=str(name[i])
            print(document)
           # print(name[i])
            
            f=open(document,'a')
            print('open鎴愬姛')
            f.write(text)
            print('write鎴愬姛')
            f.close()
            print('close鎴愬姛')
            
            driver.quit()
            driver.Dispose()
            print('quit鎴愬姛')
            
        except:
            #print(name[i]+' error')
            string=string+'\n'+name[i]+"'s abstract fail to get"
        
        time.sleep(2)
    fp=open('ErrorFile','a')
    fp.write(string)
    fp.close()
        

def main():
    
    start_url="https://epubs.siam.org/action/doSearch?"
    page=0
    Page_Number_list=[]
    field=['data+mining']
    publication=['40000050']
    sortBy=['Ppub']
    url=start_url+'AllField='+field[0]+'&publication='+publication[0]+'&target=default'+'&startPage='+str(page)+'&sortBy='+sortBy[0]
    print(url)
    title_list=[]
    author_list=[]
    publish_list=[]
    articleUrl_list=[]
    URL=url
    while URL:
        try: 
           html=get_html(URL)
        except:
           print('Timeout!!!')
        Page_Number_list.append(str(page+1))
        
        title,author,publish,articleUrl=parse(html)
        page=page+1
#https://epubs.siam.org/action/doSearch?AllField=machine+learning&publication=40000050&target=default&startPage=0&sortBy=Ppub

        URL=start_url+'AllField='+field[0]+'&publication='+publication[0]+'&target=default'+'&startPage='+str(page)+'&sortBy='+sortBy[0]
        
        if page>9:
            URL=None
        for i in range(len(title)):
            print(i)
            title_list.append(title[i])
            author_list.append(author[i])
            articleUrl_list.append(articleUrl[i])
            publish_list.append(publish[i])
        print('鍦板潃鈥斺€斺€斺€斺€斺€斺€斺€斺€斺€斺€斺€?+str(articleUrl_list[3]))
    fp1=open('Timeerror_File','a')
    fp1.write(str(Page_Number_list))
    fp1.close() 
    text=pd.DataFrame({'title':title_list,'author':author_list,'publish':publish_list,'url': articleUrl_list})
#    colunm={'name','author','publish','url'}
    text.to_csv('Ieee-machine learning new.csv',index=False)
    
    get_abstract(title_list,articleUrl_list)
"""
    page_num=1
    
    URL=url
    while URL:
        html=get_html(URL)
        name,author,publish,articleUrl=parse(html)
        page_num+=1
        URL=url+'&pageNumber='+str(page_num)

        if page_num>10:
            URL=None
        
        for i in range(len(name)):
            #print(i)
            name_list.append(name[i])
            author_list.append(author[i])
            publish_list.append(publish[i])
            articleUrl_list.append(articleUrl[i])
    
    text=pd.DataFrame({'name':name_list,'author':author_list,'publish':publish_list,'url': articleUrl_list})
#    colunm={'name','author','publish','url'}
    text.to_csv('Ieee-machine learning new.csv',index=False)
    
    get_abstract(name_list,articleUrl_list)
"""
if __name__=='__main__':
    main()
